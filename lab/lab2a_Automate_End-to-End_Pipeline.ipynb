{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](../imgs/MLU_Logo.png)\n",
    "\n",
    "---\n",
    "\n",
    "# Automate an End-to-end MLOps Pipeline\n",
    "\n",
    "Now, it is time for automating the ML pipeline using the MLOps environment.\n",
    "\n",
    "We'll do that by putting a zip file, called **trainingjob.zip**, in an S3 bucket. CodePipeline will listen to that bucket and start a job. This zip file has the following structure:\n",
    " - trainingjob.json (Sagemaker training job descriptor)\n",
    " - environment.json (Instructions to the environment of how to deploy and prepare the endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Container Image\n",
    "\n",
    "Let's start defining the hyperparameters and other attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "use_xgboost_builtin=True\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = boto3.session.Session().region_name\n",
    "model_prefix='iris-model'\n",
    "training_image = None\n",
    "hyperparameters = None\n",
    "if use_xgboost_builtin: \n",
    "    training_image = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, version='1.0-1')\n",
    "    hyperparameters = {\n",
    "        \"alpha\": 0.42495142279951414,\n",
    "        \"eta\": 0.4307531922567607,\n",
    "        \"gamma\": 1.8028358018081714,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_child_weight\": 5.925133573560345,\n",
    "        \"num_class\": 3,\n",
    "        \"num_round\": 30,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"reg_lambda\": 10,\n",
    "        \"silent\": 0,\n",
    "    }\n",
    "else:\n",
    "    training_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, model_prefix)\n",
    "    hyperparameters = {\n",
    "        \"max_depth\": 11,\n",
    "        \"n_jobs\": 5,\n",
    "        \"n_estimators\": 120\n",
    "    }\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Parameters\n",
    "\n",
    "Here, we define the parameters for training and deploying Lambda functions: `mlops-op-training` and `mlops-op-deployment`. You can check the details of functions in [**Lambda**](https://console.aws.amazon.com/lambda/home?region=us-west-2#/functions) console.\n",
    "\n",
    "#### Defining `training_params`\n",
    "\n",
    "Notice that you need to replace \"**your_alias**\" with your alias when you setup the Cloudformation pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "your_alias = \"rlhu\"\n",
    "roleArn = \"arn:aws:iam::{}:role/{}\".format(account_id, your_alias)\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = model_prefix + timestamp\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "training_params = {}\n",
    "\n",
    "# Here we set the reference for the Image Classification Docker image, stored on ECR (https://aws.amazon.com/pt/ecr/)\n",
    "training_params[\"AlgorithmSpecification\"] = {\n",
    "    \"TrainingImage\": training_image,\n",
    "    \"TrainingInputMode\": \"File\"\n",
    "}\n",
    "\n",
    "# The IAM role with all the permissions given to Sagemaker\n",
    "training_params[\"RoleArn\"] = roleArn\n",
    "\n",
    "# Here Sagemaker will store the final trained model\n",
    "training_params[\"OutputDataConfig\"] = {\n",
    "    \"S3OutputPath\": 's3://{}/{}'.format(sagemaker_session.default_bucket(), model_prefix)\n",
    "}\n",
    "\n",
    "# This is the config of the instance that will execute the training\n",
    "training_params[\"ResourceConfig\"] = {\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.m4.xlarge\",\n",
    "    \"VolumeSizeInGB\": 30\n",
    "}\n",
    "\n",
    "# The job name. You'll see this name in the Jobs section of the Sagemaker's console\n",
    "training_params[\"TrainingJobName\"] = job_name\n",
    "\n",
    "for i in hyperparameters:\n",
    "    hyperparameters[i] = str(hyperparameters[i])\n",
    "    \n",
    "# Here you will configure the hyperparameters used for training your model.\n",
    "training_params[\"HyperParameters\"] = hyperparameters\n",
    "\n",
    "# Training timeout\n",
    "training_params[\"StoppingCondition\"] = {\n",
    "    \"MaxRuntimeInSeconds\": 360000\n",
    "}\n",
    "\n",
    "# The algorithm currently only supports fully replicated model (where data is copied onto each machine)\n",
    "training_params[\"InputDataConfig\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params[\"InputDataConfig\"].append({\n",
    "    \"ChannelName\": \"train\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/input/train'.format(\n",
    "                sagemaker_session.default_bucket(), \n",
    "                model_prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\"\n",
    "})\n",
    "training_params[\"InputDataConfig\"].append({\n",
    "    \"ChannelName\": \"validation\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/input/validation'.format(\n",
    "                sagemaker_session.default_bucket(), \n",
    "                model_prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\"\n",
    "})\n",
    "training_params[\"Tags\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining `deployment_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_params = {\n",
    "    \"EndpointPrefix\": model_prefix,\n",
    "    \"DevelopmentEndpoint\": {\n",
    "        # we want to enable the endpoint monitoring\n",
    "        \"InferenceMonitoring\": True,\n",
    "        # we will collect 100% of all the requests/predictions\n",
    "        \"InferenceMonitoringSampling\": 100,\n",
    "        \"InferenceMonitoringOutputBucket\": 's3://{}/{}/monitoring/dev'.format(\n",
    "            sagemaker_session.default_bucket(), model_prefix),\n",
    "        # we don't want to enable A/B tests in development\n",
    "        \"ABTests\": False,\n",
    "        # we'll use a basic instance for testing purposes\n",
    "        \"InstanceType\": \"ml.t2.large\", # \"ml.t3.medium\" does not work\n",
    "        \"InitialInstanceCount\": 1,\n",
    "        # we don't want high availability/escalability for development\n",
    "        \"AutoScaling\": None\n",
    "    },\n",
    "    \"ProductionEndpoint\": {\n",
    "        # we want to enable the endpoint monitoring\n",
    "        \"InferenceMonitoring\": True,\n",
    "        # we will collect 100% of all the requests/predictions\n",
    "        \"InferenceMonitoringSampling\": 100,\n",
    "        \"InferenceMonitoringOutputBucket\": 's3://{}/{}/monitoring/prd'.format(\n",
    "            sagemaker_session.default_bucket(), model_prefix),\n",
    "        # we want to do A/B tests in production\n",
    "        \"ABTests\": True,\n",
    "        # we'll use a better instance for production. CPU optimized\n",
    "        \"InstanceType\": \"ml.c5.xlarge\", # \"ml.m4.xlarge\", # \n",
    "        \"InitialInstanceCount\": 2,\n",
    "        \"InitialVariantWeight\": 0.1,\n",
    "        # we want elasticity. at minimum 2 instances to support the endpoint and at maximum 10\n",
    "        # we'll use a threshold of 200 predictions per instance to start adding new instances or remove them\n",
    "        \"AutoScaling\": {\n",
    "            \"MinCapacity\": 2,\n",
    "            \"MaxCapacity\": 10,\n",
    "            \"TargetValue\": 200.0,\n",
    "            \"ScaleInCooldown\": 30,\n",
    "            \"ScaleOutCooldown\": 60,\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing and uploading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 419 ms, sys: 33 ms, total: 452 ms\n",
      "Wall time: 608 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "iris = datasets.load_iris()\n",
    "prefix='mlops/iris'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.33, random_state=42, stratify=iris.target)\n",
    "np.savetxt(\"iris_train.csv\", np.column_stack((y_train, X_train)), delimiter=\",\", fmt='%0.3f')\n",
    "np.savetxt(\"iris_test.csv\", np.column_stack((y_test, X_test)), delimiter=\",\", fmt='%0.3f')\n",
    "\n",
    "# Upload the dataset to an S3 bucket\n",
    "input_train = sagemaker_session.upload_data(path='iris_train.csv', key_prefix='%s/input/train' % model_prefix)\n",
    "input_test = sagemaker_session.upload_data(path='iris_test.csv', key_prefix='%s/input/validation' % model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activating the Pipeline \n",
    "\n",
    "Alright! Now it's time to start the end-to-end training/deployment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket_name : mlops-us-east-1-576192184325\n",
      "key_name : training_jobs/iris-model/trainingjob.zip\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import io\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "sts_client = boto3.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "bucket_name = \"mlops-%s-%s\" % (region, account_id)\n",
    "print(\"bucket_name : {}\".format(bucket_name))\n",
    "key_name = \"training_jobs/%s/trainingjob.zip\" % model_prefix\n",
    "print(\"key_name : {}\".format(key_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's activate our pipeline by putting the training data into the [**S3 bucket**](https://s3.console.aws.amazon.com/s3/buckets/mlops-us-west-2-901591081018?region=us-west-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'SMWVFHB9WC36MBD6',\n",
       "  'HostId': 'uo7iIvIVT7hS0MLwEUdyYJGALVo699JmEx08h1fULDtCph0bJgVED9BF1JAv4n9SNf3ykHSqaCw=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'uo7iIvIVT7hS0MLwEUdyYJGALVo699JmEx08h1fULDtCph0bJgVED9BF1JAv4n9SNf3ykHSqaCw=',\n",
       "   'x-amz-request-id': 'SMWVFHB9WC36MBD6',\n",
       "   'date': 'Mon, 08 Mar 2021 04:15:14 GMT',\n",
       "   'x-amz-version-id': 'h6CG0hWwWV4vmXJLBglAUci7ULVXgsK9',\n",
       "   'etag': '\"76d4655fcd7dad56f07cf3e34ed40033\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"76d4655fcd7dad56f07cf3e34ed40033\"',\n",
       " 'VersionId': 'h6CG0hWwWV4vmXJLBglAUci7ULVXgsK9'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_buffer = io.BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'a') as zf:\n",
    "    zf.writestr('trainingjob.json', json.dumps(training_params))\n",
    "    zf.writestr('deployment.json', json.dumps(deployment_params))\n",
    "zip_buffer.seek(0)\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=key_name, Body=bytearray(zip_buffer.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring MLOps Pipeline\n",
    "\n",
    "After the s3 bucket get the new data, the MLOps pipeline will be kicked off automatically. It involved the following functions:\n",
    "- `TrianModel`: an AWS Cloud Formation stack to train (see in below picture);\n",
    "- `DeployDev`: an AWS Cloud Formation stack to deploy trained model to **Development** environment (see in below picture);\n",
    "- `DeployApproval`: an action you need to take in either by clicking `DeployApproval` in Codepipeline or in next notebook;\n",
    "\n",
    "<img src=\"../imgs/codepipeline_approval.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "- `DeployProd`: an AWS Cloud Formation stack to deploy trained model to **Production** environment (see in below picture);\n",
    "\n",
    "While the pipeline is built automatically, open the CodePipeline console to see the status of our building pipeline. After around 20 minutes, the finished building pipeline is showing as below:\n",
    "\n",
    "<img src=\"../imgs/codepipeline.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Action:** Now, click on [the next NOTEBOOK](lab2b_Productionizing_End-to-end_Pipeline.ipynb) to see the progress and test your endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Resources\n",
    "\n",
    "## A/B TESTS\n",
    "\n",
    "If you take a look on the **deployment** parameters you'll see that we enabled the **Production** endpoint for A/B tests. To try this, just deploy the first model into production, then run the section **1.3** again. Feel free to change some hyperparameter values in the section **1.1** before starting a new training session.\n",
    "\n",
    "When publishing the second model into **Development**, the endpoint will be updated and the model will be replaced without compromising the user experience. This is the natural behavior of an Endpoint in SageMaker when you update it.\n",
    "\n",
    "After you approve the deployment into **Production**, the endponint will be updated and a second model will be added to it. Now it's time to execute some **A/B tests**. In the **Progress** Jupyter (link above), execute the last cell (test code) to show which model answered your request. You just need to keep sending some requests to see the **Production** endpoint using both models A and B, respecting the proportion defined by the variable **InitialVariantWeight** in the deployment params.\n",
    "\n",
    "In a real life scenario you can monitor the performance of both models and then adjust the **Weight** of each model to do the full transition to the new model (and remove the old one) or to rollback the new deployment.\n",
    "\n",
    "To adjust the weight of each model (Variant Name) in an endpoint, you just need to call the following function: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.update_endpoint_weights_and_capacities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](../imgs/MLU_Logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
